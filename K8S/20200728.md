2020.07.28
----------

RAID 복습
========
1) 목적
* Fault Tolerance 극복을 위해
* 
2) 구성
* 하드웨어 기반: RAID controller, 칩셋 자체지원 등
* 소프트웨어 기반: lvcreate, mdadm 등
* RAID를 위한 하드웨어, 소프트웨어는 대체제가 아닌 보완제

3) RAID 종류
* RAID 0
> 데이터를 청크단위로 나누어, 2개 이상의 디스크에 나눠서 담는다.
> Stripe라고도 불림
> 전송속도 개선 가능
> 데이터를 나누어 담기 때문에 디스크 하나에 이상이 생기면, 해당하는 데이터 손상된다. (보존이 아닌 퍼포먼스 극대화에 중점을 둔 기술)

* RAID 1
> 각 데이터를 복제하여 2개 이상의 디스크에 나누어 담는다. 
> Mirror라고도 불림
> 읽기 속도 개선 / 쓰기 속도 그대로 (보존 및 HA에 중점을 둔 기술)
> 공간의 효율성 떨어짐 (총 용량이 /n 단위로 감소함)

* RAID 2 (잘 안쓰임)
> Hamming Code를 사용하는 방식
> Bit Stripe라고도 불림
> Dedicated Parity 사용 (디스크 복구 용도)
> 특정 데이터를 1개의 Parity와 다수의 복제본으로 2개 이상의 디스크에 나누어 담음
> Parity가 XOR(Exclusive OR) 사용하여 디스크를 복구하는 형식

* RAID 3(잘 안쓰임)
> Byte Stripe라고도 불림

* RAID 4(잘 안쓰임)
> Block Stripe라고도 불림

* RAID 5
> Stripe
> Distributed Parity 사용
> 디스크 손상 시 Parity 복구에 따른 부하를 감소시킬 수 있다. (디스크 4개 사용, 디스크 1개 손상 시 1/4확률로 1개의 Parity는 계산할 필요가 없음)

* RAID 6
> Stripe
> 2개의 Distributed Parity 사용

4) Nested RAID
* 기존 RAID를 쓰까쓰는 형태
ex) RAID10(RAID1+RAID0)

5) 기존 RAID의 문제점(?)
* 원격에 있는 디스크는 RAID구성이 불가능함
* 이를 보완하기 위해 CEPH 스토리지 등장

CEPH 스토리지
===========
1) 소개 
* SDS (Software Defined Storage)
* RADOS라고도 불림
* Block/Object/File 스토리지 전부 지원 (RADOS Cluster)
<img src=https://docs.ceph.com/docs/firefly/_images/stack.png>
> CEPH 스토리지 클러스터 구성

2) CEPH 스토리지 구성요소
* OSD(Object Store Device)
> 디스크 장치 관리
* MON (Monitor Node)
> 클러스터 관리
* MGR (Manager)
> 클러스터 관리 (중요 x)
* MDS (Metadata Servre)
> 오브젝트에 메타데이터 제공 기능 (파일=오브젝트+메타데이터)
* RGW (RADOS Gateway)
> REST API를 제공하기 위한 웹서버

* OSD+MON => Block Storage 제공 가능
* OSD+MON+MDS => File Storage 제공 가능
* ODS+MON+MDS+RGW => Object Storage 제공 가능
